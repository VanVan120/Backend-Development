{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7082c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports & Setup\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b90bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Definition (Identical to Master for Compatibility)\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.upsample(self.conv(x))\n",
    "\n",
    "class OSCCMultiTaskModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Backbone: DenseNet169\n",
    "        # Updated to use 'weights' instead of deprecated 'pretrained'\n",
    "        self.backbone = models.densenet169(weights=models.DenseNet169_Weights.DEFAULT)\n",
    "        num_ftrs = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # --- HEADS (All must exist to match state_dict) ---\n",
    "        self.head_tvnt = nn.Sequential(nn.Linear(num_ftrs, 256), nn.ReLU(), nn.Dropout(0.2), nn.Linear(256, 2))\n",
    "        self.head_poi = nn.Sequential(nn.Linear(num_ftrs, 256), nn.ReLU(), nn.Dropout(0.2), nn.Linear(256, 5))\n",
    "        self.head_pni = nn.Sequential(nn.Linear(num_ftrs, 256), nn.ReLU(), nn.Dropout(0.2), nn.Linear(256, 2))\n",
    "        self.head_tb = nn.Sequential(nn.Linear(num_ftrs, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "        self.head_mi = nn.Sequential(nn.Linear(num_ftrs, 128), nn.ReLU(), nn.Linear(128, 1))\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            UpsampleBlock(num_ftrs, 512), UpsampleBlock(512, 256),\n",
    "            UpsampleBlock(256, 128), UpsampleBlock(128, 64),\n",
    "            UpsampleBlock(64, 32), nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        # --- NEW: PNI Segmentation Decoder ---\n",
    "        self.decoder_pni = nn.Sequential(\n",
    "            UpsampleBlock(num_ftrs, 512), UpsampleBlock(512, 256),\n",
    "            UpsampleBlock(256, 128), UpsampleBlock(128, 64),\n",
    "            UpsampleBlock(64, 32), nn.Conv2d(32, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.features(x)\n",
    "        pooled = F.relu(features, inplace=True)\n",
    "        pooled = F.adaptive_avg_pool2d(pooled, (1, 1))\n",
    "        pooled = torch.flatten(pooled, 1)\n",
    "        \n",
    "        return {\n",
    "            'tvnt': self.head_tvnt(pooled),\n",
    "            'poi': self.head_poi(pooled),\n",
    "            'pni': self.head_pni(pooled),\n",
    "            'tb': self.head_tb(pooled),\n",
    "            'mi': self.head_mi(pooled),\n",
    "            'doi': self.decoder(features),\n",
    "            'pni_seg': self.decoder_pni(features)\n",
    "        }\n",
    "\n",
    "print(\"Model Architecture Defined (Compatible with Inference).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb3ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset Loader (Folder-Based for Kaggle Dataset)\n",
    "\n",
    "class OSCCBinaryDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # 1. Define Class Mapping\n",
    "        # The code looks for these keywords in the folder names to assign labels.\n",
    "        # 0 = Normal, 1 = Cancer (OSCC)\n",
    "        self.class_keywords = {\n",
    "            'normal': 0,\n",
    "            'oscc': 1,\n",
    "            'tumor': 1,\n",
    "            'cancer': 1\n",
    "        }\n",
    "        \n",
    "        if not os.path.exists(root_dir):\n",
    "            print(f\"‚ùå Dataset root '{root_dir}' not found!\")\n",
    "            return\n",
    "\n",
    "        # 2. Auto-Discovery of Images\n",
    "        print(f\"Scanning '{root_dir}' for images...\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):\n",
    "                    # Determine label from parent folder name\n",
    "                    folder_name = os.path.basename(root).lower()\n",
    "                    label = None\n",
    "                    \n",
    "                    # Check if folder name contains any of our keywords\n",
    "                    for keyword, val in self.class_keywords.items():\n",
    "                        if keyword in folder_name:\n",
    "                            label = val\n",
    "                            break\n",
    "                    \n",
    "                    # If we found a valid label, add the image\n",
    "                    if label is not None:\n",
    "                        self.samples.append((os.path.join(root, file), label))\n",
    "        \n",
    "        # Summary\n",
    "        if len(self.samples) == 0:\n",
    "            print(\"‚ö†Ô∏è No images found! Check your folder structure.\")\n",
    "        else:\n",
    "            # Count classes\n",
    "            labels = [s[1] for s in self.samples]\n",
    "            num_normal = labels.count(0)\n",
    "            num_cancer = labels.count(1)\n",
    "            print(f\"‚úÖ Loaded {len(self.samples)} images.\")\n",
    "            print(f\"   - Normal: {num_normal}\")\n",
    "            print(f\"   - Cancer (OSCC): {num_cancer}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224)) # Fallback black image\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Update this path to where you unzipped the Kaggle dataset\n",
    "DATASET_ROOT = \"/root/.cache/kagglehub/datasets/ashenafifasilkebede/dataset/versions/1\"\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# --- Advanced Augmentation ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Data Splitting ---\n",
    "full_dataset = OSCCBinaryDataset(DATASET_ROOT, transform=None)\n",
    "\n",
    "if len(full_dataset) > 0:\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    # Create indices\n",
    "    indices = list(range(len(full_dataset)))\n",
    "    random.shuffle(indices)\n",
    "    train_idx = indices[:train_size]\n",
    "    val_idx = indices[train_size:]\n",
    "    \n",
    "    # Create separate dataset instances for transforms\n",
    "    train_ds = OSCCBinaryDataset(DATASET_ROOT, transform=train_transform)\n",
    "    val_ds = OSCCBinaryDataset(DATASET_ROOT, transform=val_transform)\n",
    "    \n",
    "    # Loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=torch.utils.data.SubsetRandomSampler(train_idx))\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, sampler=torch.utils.data.SubsetRandomSampler(val_idx))\n",
    "    \n",
    "    print(f\"‚úÖ Data Split: {len(train_idx)} Training, {len(val_idx)} Validation\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset empty. Please upload the Kaggle dataset folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Advanced Training Loop (With Validation, Best Model Saving & Early Stopping)\n",
    "from tqdm.auto import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "model = OSCCMultiTaskModel().to(DEVICE)\n",
    "\n",
    "# Resume if exists\n",
    "if os.path.exists(\"model_a.pth\"):\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(\"model_a.pth\", map_location=DEVICE))\n",
    "        print(\"‚úÖ Loaded existing weights.\")\n",
    "    except:\n",
    "        print(\"üÜï Starting fresh.\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "# Scheduler: Reduce LR if no improvement for 5 epochs\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "EARLY_STOPPING_PATIENCE = 15 # Stop if no improvement for 15 epochs\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(f\"üöÄ Starting Training for {NUM_EPOCHS} Epochs (Early Stopping: {EARLY_STOPPING_PATIENCE})...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # --- TRAIN ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    # Wrap train_loader with tqdm for progress bar\n",
    "    train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n",
    "    \n",
    "    for images, labels in train_loop:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs['tvnt'], labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, preds = torch.max(outputs['tvnt'], 1)\n",
    "        train_correct += torch.sum(preds == labels.data)\n",
    "        train_total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        train_loop.set_postfix(loss=loss.item())\n",
    "        \n",
    "    epoch_train_loss = train_loss / train_total\n",
    "    epoch_train_acc = train_correct.double() / train_total\n",
    "\n",
    "    # --- VALIDATE ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    # Wrap val_loader with tqdm\n",
    "    val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\", leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loop:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['tvnt'], labels)\n",
    "            \n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, preds = torch.max(outputs['tvnt'], 1)\n",
    "            val_correct += torch.sum(preds == labels.data)\n",
    "            val_total += labels.size(0)\n",
    "            \n",
    "    epoch_val_loss = val_loss / val_total\n",
    "    epoch_val_acc = val_correct.double() / val_total\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f} | LR: {current_lr:.1e}\")\n",
    "    \n",
    "    # Scheduler Step\n",
    "    scheduler.step(epoch_val_acc)\n",
    "    \n",
    "    # Save Best Model & Early Stopping Logic\n",
    "    if epoch_val_acc > best_val_acc:\n",
    "        best_val_acc = epoch_val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), \"model_a_best.pth\")\n",
    "        print(f\"  üåü New Best Model Saved! (Acc: {best_val_acc:.4f})\")\n",
    "        epochs_no_improve = 0 # Reset counter\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"  ‚è≥ No improvement for {epochs_no_improve}/{EARLY_STOPPING_PATIENCE} epochs.\")\n",
    "        \n",
    "    if epochs_no_improve >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\nüõë Early Stopping triggered! No improvement for {EARLY_STOPPING_PATIENCE} epochs.\")\n",
    "        break\n",
    "\n",
    "print(f\"üèÅ Training Complete. Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Load best weights before final save\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save(model.state_dict(), \"model_a.pth\")\n",
    "print(\"‚úÖ Final 'model_a.pth' updated with best weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save Model\n",
    "torch.save(model.state_dict(), \"model_a.pth\")\n",
    "print(\"‚úÖ Model saved to model_a.pth\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
